{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scaling_sweep",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obrq1E8wL37i"
      },
      "source": [
        "! pip install wandb\n",
        "# grab @TheodoreGalanos's fork for config file\n",
        "! git clone https://github.com/TheodoreGalanos/gpt-neox/\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCVZqoc6MDCs"
      },
      "source": [
        "# initialize config\n",
        "sweep_config = {\n",
        "  \"name\": \"Scaling laws sweep\",\n",
        "  \"method\": \"grid\",\n",
        "  \"parameters\": {\n",
        "    \"valid_set\": {\n",
        "        \"distribution\": \"categorical\",\n",
        "        'values': [\n",
        "          ### This will be a list of strings\n",
        "          ### with the following values:\n",
        "          ### N, num_layers, num_attention_heads, hidden_size, lr\n",
        "          ### separated by commas\n",
        "      ]\n",
        "    },\n",
        "  }\n",
        "}\n",
        "\n",
        "# read TG's config\n",
        "df = pd.read_csv(\n",
        "    './gpt-neox/configs/scaling_experiment/config_parameters.csv',\n",
        "    )\n",
        "\n",
        "# create strings for each row\n",
        "for i in range(len(df)):\n",
        "  row = df.iloc[i]\n",
        "  row['N'] = 12*row['n_layer']*row['n_embd']**2\n",
        "  # add LR according equation D.1 from Kaplan et. al\n",
        "  # \"Scaling Laws for Neural Language Models\"\n",
        "  row['lr'] = 0.003239 + (-0.0001395)*np.log(row['N'])\n",
        "  row.drop(['Exponent', 'head_state'], inplace=True)\n",
        "  row = row[['N', 'n_layer', 'n_head', 'n_embd', 'lr']]\n",
        "  # concatenate into a string to be split later\n",
        "  row = ','.join([str(x) for x in row])\n",
        "  sweep_config['parameters']['valid_set']['values'].append(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1D3k39pPrp_"
      },
      "source": [
        "# check source\n",
        "print(df)\n",
        "\n",
        "# and config\n",
        "sweep_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae4Edr-jMAYk"
      },
      "source": [
        "# test run\n",
        "sweep_id = wandb.sweep(sweep_config)\n",
        "\n",
        "def train():\n",
        "    run = wandb.init()\n",
        "    print(run.config.valid_set)\n",
        "    vars = {k:v for k,v in zip(\n",
        "        # these are from neox_arguments.md\n",
        "        ['N',\n",
        "         'num_layers',\n",
        "         'num_attention_heads',\n",
        "         'hidden_size',\n",
        "         'lr'\n",
        "         ],\n",
        "        [float(x) for x in run.config.valid_set.split(',')]\n",
        "    )}\n",
        "    print([x for x in zip(vars.keys(),vars.values())])\n",
        "    run.finish()\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config)\n",
        "agent = wandb.agent(sweep_id=sweep_id, function=train)\n",
        "agent.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}