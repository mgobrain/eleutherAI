{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sweep_neox_args",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Miv7__8gB9"
      },
      "source": [
        "! pip install wandb\n",
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntqqJYZNRz5w"
      },
      "source": [
        "# [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)\n",
        "\n",
        "Most of this is from Table 2.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmTLoMY3BNMv"
      },
      "source": [
        "sweep_config = {\n",
        "    'name': 'GPT-3 grid sweep',\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "        'name': 'loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'n_params': {\n",
        "            # total number of trainable parameters\n",
        "            'values': [\n",
        "                       125e6, 350e6, 760e6,\n",
        "                       1.3e9, 2.7e9, 6.7e9,\n",
        "                       13e9, 175e9\n",
        "            ]\n",
        "        },\n",
        "        'num_layers': {\n",
        "            # n_layer\n",
        "            # total number of layers\n",
        "            'values': [\n",
        "                       12,24,32,40,96, # Table 2.1\n",
        "            ]\n",
        "        },\n",
        "        'hidden_size': {\n",
        "            # d_model\n",
        "            # number of units in each bottleneck layer\n",
        "            # n.b. feedforward layer is 4x bottleneck\n",
        "            # d_ff = 4*d_model\n",
        "            'values': [\n",
        "                       768,1024,1536,2048,2560, # Table 2.1\n",
        "                       4096,5140,12288, # Table 2.1\n",
        "            ]\n",
        "        },\n",
        "        'attn_dim': {\n",
        "            # d_head\n",
        "            # dimension of each attention head\n",
        "            'values': [\n",
        "                       64,80,96,128, # Table 2.1\n",
        "            ]\n",
        "        },\n",
        "        'n_ctx': {\n",
        "            # context window in tokens, fixed\n",
        "            'values': [2048]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            # in tokens\n",
        "            # \"We also gradually increase the batch size \n",
        "            # linearly from a small value (32k tokens) to\n",
        "            # the full value over the first 4-12 billion\n",
        "            # tokens of training, depending on the model size\"\n",
        "            'values': [\n",
        "                       0.5e6, 1e6, 2e6, 3.2e6\n",
        "            ]\n",
        "        },\n",
        "        'lr': {\n",
        "            # learning_rate\n",
        "            # There is a linear LR warmup over the first 375\n",
        "            # million tokens.\n",
        "\n",
        "            # ...we use cosine decay for learning rate down\n",
        "            # to 10% of its value, over 260 billion tokens\n",
        "            'values': [\n",
        "                       6e-4, 3e-4, 2.5e-4, 2e-4,\n",
        "                       1.6e-4, 1.2e-4, 1e-4, 0.6e-4\n",
        "            ]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            # Appendix B: Details of Model Training\n",
        "            # beta_1 = 0.9, beta_2 = 0.95, epsilon = 1e-8\n",
        "            'values': ['adam']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# test run\n",
        "sweep_id = wandb.sweep(sweep_config)\n",
        "\n",
        "def train():\n",
        "    run = wandb.init()\n",
        "    print(run.config)\n",
        "    run.finish()\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config)\n",
        "agent = wandb.agent(sweep_id=sweep_id, function=train)\n",
        "agent.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06j3Wa0F6w9w"
      },
      "source": [
        "sweep_config = {\n",
        "    'name': \"GPT-3 presets sweep\",\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "        'name': 'loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'presets': {\n",
        "            'values': [\n",
        "                       # Each entry here will be a string of values\n",
        "                       # separated by commas\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "table = [\n",
        "  # this is Table 2.1 as a list of tuples\n",
        "  # (n_params, n_layers, d_model, n_heads, d_head, batch_size, learning_rate)\n",
        "  (125e6, 12, 768, 12, 64, 0.5e6, 6e-4), # GPT-3 small\n",
        "  (350e6, 24, 1024, 16, 64, 0.5e6, 3e-4), # GPT-3 medium\n",
        "  (760e6, 24, 1536, 16, 96, 0.5e6, 2.5e-4), # GPT-3 large\n",
        "  (1.3e9, 24, 2048, 24, 128, 1e6, 2e-4), # GPT-3 XL\n",
        "  (2.7e9, 32, 2560, 32, 80, 1e6, 1.6e-4), # GPT-3 2.7B\n",
        "  (6.7e9, 32, 4096, 32, 128, 2e6, 1.2e-4), # GPT-3 6.7B\n",
        "  (13e9, 40, 5140, 40, 128, 2e6, 1e-4), # GPT-3 13B\n",
        "  (175e9, 96, 12288, 96, 128, 3.2e6, 0.6e-4), # \"GPT-3\" (175B)\n",
        "]\n",
        "\n",
        "for i in range(len(table)):\n",
        "  sweep_config['parameters']['presets']['values'].append(\n",
        "      ','.join([str(x) for x in table[i]])\n",
        "  )\n",
        "\n",
        "# test run\n",
        "sweep_id = wandb.sweep(sweep_config)\n",
        "\n",
        "def train():\n",
        "    run = wandb.init()\n",
        "    print(run.config.presets)\n",
        "    vars = {k:v for k,v in zip(\n",
        "        # these are from neox_arguments.md\n",
        "        ['N',\n",
        "         'num_layers', # \"n_layers\" (GPT)\n",
        "         'hidden_size', # \"d_model\" (GPT)\n",
        "         'num_attention_heads' # \"n_heads\" (GPT)\n",
        "         'attn_dim' # \"d_head (GPT)\"; doesn't appear to be used in neox?\n",
        "         'batch_size' # same?\n",
        "         'lr' # \"learning_rate\" (GPT)\n",
        "         ],\n",
        "        [float(x) for x in run.config.presets.split(',')]\n",
        "    )}\n",
        "    print([x for x in zip(vars.keys(),vars.values())])\n",
        "    run.finish()\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config)\n",
        "agent = wandb.agent(sweep_id=sweep_id, function=train)\n",
        "agent.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JWxG3QvrmzA"
      },
      "source": [
        "# [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSHIltF-pvie"
      },
      "source": [
        "sweep_config = {\n",
        "    'metric': {\n",
        "      'name': 'loss',\n",
        "      'goal': 'minimize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'D': {\n",
        "            # dataset size in tokens\n",
        "            'values': [\n",
        "                21e6, 43e6, 86e6, 172e6, 344e6, 688e6,\n",
        "                1.400e9, 2.200e9\n",
        "                # from Figure 9\n",
        "                ]\n",
        "        },\n",
        "        \"\"\"\n",
        "        BACKGROUND AND METHODS\n",
        "        \"\"\"\n",
        "        'n_vocab': {\n",
        "            # vocabulary size\n",
        "            'values': [50257]\n",
        "        },\n",
        "\n",
        "        \"\"\"\n",
        "        PARAMETER AND COMPUTE SCALING OF TRANSFORMERS\n",
        "\n",
        "        N.b. model size is number of non-embedding parameters\n",
        "        N = 12*n_layer*d_model**2\n",
        "        where d_model = d_attn = 0.25 * d_ff\n",
        "        \"\"\"\n",
        "        # see also \"shape\"\n",
        "        'n_layers':{\n",
        "            # number of layers\n",
        "            'values': []\n",
        "        },\n",
        "        'd_model': {\n",
        "            # dimension of residual stream\n",
        "            # from Figure 5\n",
        "            'values': [256, 512, 1024]\n",
        "        },\n",
        "        'd_ff': {\n",
        "            # dimension of intermediate feed-forward layer\n",
        "            # it seems like this is 4*d_attn?\n",
        "            'values': []\n",
        "        },\n",
        "        'd_attn': {\n",
        "            # dimension of attention output\n",
        "            # it seems like this is equal to d_model?\n",
        "            'values': []\n",
        "        },\n",
        "        'n_heads': {\n",
        "            #number of attention heads per layer\n",
        "            # Figure 5\n",
        "            'values': [8]\n",
        "        },\n",
        "        'n_ctx': {\n",
        "            # number of tokens in input context\n",
        "            # \"1024 for most runs, though we also experiment with shorter contexts\"\n",
        "            'values': [1024]\n",
        "        },\n",
        "\n",
        "        \"\"\"\n",
        "        TRAINING PROCEDURES\n",
        "        \"\"\"\n",
        "        'optimizer': {\n",
        "            'values': ['adam',\n",
        "                       'adafactor' # for \"largest models\"\n",
        "                       ]\n",
        "        },\n",
        "        'adam_steps': {\n",
        "            'values': [2.5e5]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            # each sequence is 1024 tokens\n",
        "            # \"2**19 for most runs, but we also vary it to measure the critical batch size\"\n",
        "            # \"Note that in these results the batch size B remains fixed for all models\"\n",
        "            'values': [512]\n",
        "        },\n",
        "\n",
        "        \"\"\"\n",
        "        SHAPE - tuple of (n_layer, d_model)\n",
        "        \"\"\"\n",
        "        'shape': {\n",
        "            'values': [\n",
        "                       (2,128), # \"small\"\n",
        "                       (6,4288),\n",
        "                       (207,768),\n",
        "                       (36,1280),\n",
        "                       (48,1600), # from Radford 2019 (multi-shot paper)\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}